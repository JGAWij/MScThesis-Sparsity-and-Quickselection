{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1dqgGJBf-sG"
      },
      "source": [
        "# **1. Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maVTZCbuf3wb"
      },
      "outputs": [],
      "source": [
        "target_gini = 0.0\n",
        "#target_gini ranges from 0 till 0.8 in steps of 0.1\n",
        "dataset_name = 'MNIST'\n",
        "\n",
        "epoch = 10\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g7UjcLNSgz7"
      },
      "source": [
        "# **1B. Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-kBt3WgA7Qd",
        "outputId": "5c003c28-aebf-419c-e739-17b3c3261370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLQu4vU1d9aG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import dok_matrix\n",
        "#the \"sparseoperations\" Cython library was tested in Ubuntu 16.04. Please note that you may encounter some \"solvable\" issues if you compile it in Windows.\n",
        "#import sparseoperations\n",
        "import datetime\n",
        "\n",
        "from scipy import sparse\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import loadmat\n",
        "from scipy.io import savemat\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import urllib.request as urllib2\n",
        "import errno\n",
        "import os\n",
        "import sys; sys.path.append(os.getcwd())\n",
        "import argparse\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment #The linear_assignment function is deprecated in 0.21 and will be removed from 0.23, but sklearn.utils.linear_assignment_ can be replaced by scipy.optimize.linear_sum_assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWEnqMPCX8ah"
      },
      "outputs": [],
      "source": [
        "if dataset_name == 'coil20':\n",
        "    # Set the path to the directory containing the PNG images\n",
        "    png_dir = \"/content/gdrive/MyDrive/datasets/Coil-20\"\n",
        "    # Initialize empty lists for images and labels\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over the PNG files in the directory\n",
        "    for file_name in os.listdir(png_dir):\n",
        "        if file_name.endswith(\".png\"):\n",
        "            # Load the image\n",
        "            image = Image.open(os.path.join(png_dir, file_name))\n",
        "            # Convert the image to a numerical array\n",
        "            image_array = np.array(image)\n",
        "            # Append the image array to the images list\n",
        "            images.append(image_array)\n",
        "            # Extract the class label from the file name\n",
        "            label = file_name.split(\"__\")[0]  # Assuming the label is separated by double underscores (__) from the rest of the file name\n",
        "            # Append the label to the labels list\n",
        "            labels.append(label)\n",
        "\n",
        "    # Create a dictionary to store the data\n",
        "    data = {\"images\": images, \"labels\": labels}\n",
        "\n",
        "    # Save the data as a .mat file\n",
        "    savemat(\"/content/gdrive/MyDrive/datasets/Coil20.mat\", data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPKZdvJlclRJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VifsC4-DfXYs"
      },
      "source": [
        "# **2A. Gini**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC4Cbc9ggaDb"
      },
      "source": [
        "This code is written by Jesse Wijlhuizen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMHwhwMffXln"
      },
      "outputs": [],
      "source": [
        "def calculate_gini(x):\n",
        "#given a certain distribution, this function calculates the gini coefficient\n",
        "   total = 0\n",
        "   for i, xi in enumerate(x[:-1], 1):\n",
        "      total += np.sum(np.abs(xi - x[i:]))\n",
        "      return total / (len(x) ** 2 * np.mean(x))\n",
        "\n",
        "def gini_algorithm(target_gini, num_categories):\n",
        "    # target_gini is the gini to be reached with the eventual distribution, num_categories is the number of categories in the eventual distribution\n",
        "    print(\"Start gini-process\")\n",
        "    # Set the initial lower and upper bounds for the first element\n",
        "    lower_bound = 0\n",
        "    upper_bound = 100\n",
        "\n",
        "    # Use binary search to find the appropriate value for the first element\n",
        "    while True:\n",
        "        mid = (lower_bound + upper_bound) / 2\n",
        "        x = np.array([mid] + [1] * (num_categories - 1))\n",
        "        gini_coeff = calculate_gini(x)\n",
        "\n",
        "        if abs(gini_coeff - target_gini) < 1e-3:\n",
        "            distribution = x / np.sum(x)\n",
        "            print(\"Relative distribution over num_categories:\", distribution)\n",
        "            gini_coeff = calculate_gini(distribution)\n",
        "            print(\"Gini coefficient:\", gini_coeff)\n",
        "            return gini_coeff, distribution\n",
        "\n",
        "        elif gini_coeff > target_gini:\n",
        "            upper_bound = mid\n",
        "        else:\n",
        "            lower_bound = mid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B83HaKBveZcq"
      },
      "source": [
        "# **Other Classes.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpEsqIRqo4y7"
      },
      "source": [
        "This code is an adjustment of Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heRc6AUSeUut"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
        "\n",
        "class tanh:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        return (np.tanh(z))\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        return 1 - tanh.activation(z) ** 2\n",
        "\n",
        "class Relu:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        z[z < 0] = 0\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        z[z < 0] = 0\n",
        "        z[z > 0] = 1\n",
        "        return z\n",
        "\n",
        "class LeakyRelu:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        z[z < 0] *= 0.01\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        z[z < 0] = 0.01\n",
        "        z[z > 0] = 1\n",
        "        return z\n",
        "\n",
        "\n",
        "class Linear:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        return 1\n",
        "\n",
        "\n",
        "\n",
        "class MSE:\n",
        "    def __init__(self, activation_fn=None):\n",
        "        \"\"\"\n",
        "        :param activation_fn: Class object of the activation function.\n",
        "        \"\"\"\n",
        "        if activation_fn:\n",
        "            self.activation_fn = activation_fn\n",
        "        else:\n",
        "            self.activation_fn = NoActivation\n",
        "\n",
        "    def activation(self, z):\n",
        "        return self.activation_fn.activation(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: (array) One hot encoded truth vector.\n",
        "        :param y_pred: (array) Prediction vector\n",
        "        :return: (flt)\n",
        "        \"\"\"\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(y_true, y_pred):\n",
        "        return y_pred - y_true\n",
        "\n",
        "    def delta(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Back propagation error delta\n",
        "        :return: (array)\n",
        "        \"\"\"\n",
        "        return self.prime(y_true, y_pred) * self.activation_fn.prime(y_pred)\n",
        "\n",
        "\n",
        "class NoActivation:\n",
        "    \"\"\"\n",
        "    This is a plugin function for no activation.\n",
        "    f(x) = x * 1\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        \"\"\"\n",
        "        :param z: (array) w(x) + b\n",
        "        :return: z (array)\n",
        "        \"\"\"\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(z):\n",
        "        \"\"\"\n",
        "        The prime of z * 1 = 1\n",
        "        :param z: (array)\n",
        "        :return: z': (array)\n",
        "        \"\"\"\n",
        "        return np.ones_like(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEY4xdVfedDs"
      },
      "source": [
        "# **Sparse DAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jT_WRhozSn"
      },
      "source": [
        "This code is an adjustment of Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XeeOq0Gd9jx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def backpropagation_updates_Numpy(a, delta, rows, cols, out):\n",
        "    for i in range(out.shape[0]):\n",
        "        s = 0\n",
        "        for j in range(a.shape[0]):\n",
        "            s += a[j, rows[i]] * delta[j, cols[i]]\n",
        "        out[i] = s / a.shape[0]\n",
        "\n",
        "def find_first_pos(array, value):\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return idx\n",
        "\n",
        "def find_last_pos(array, value):\n",
        "    idx = (np.abs(array - value))[::-1].argmin()\n",
        "    return array.shape[0] - idx\n",
        "\n",
        "def createSparseWeights(epsilon, noRows, noCols):\n",
        "    # generate an Erdos Renyi sparse weights mask\n",
        "    weights = lil_matrix((noRows, noCols))\n",
        "    for i in range(epsilon * (noRows + noCols)):\n",
        "        weights[np.random.randint(0, noRows), np.random.randint(0, noCols)] = np.float64(np.random.randn() / 10)\n",
        "    print(\"Create sparse matrix with \", weights.getnnz(), \" connections and \",\n",
        "          (weights.getnnz() / (noRows * noCols)) * 100, \"% density level\")\n",
        "    weights = weights.tocsr()\n",
        "    return weights\n",
        "\n",
        "def array_intersect(A, B):\n",
        "    # this are for array intersection\n",
        "    nrows, ncols = A.shape\n",
        "    dtype = {'names': ['f{}'.format(i) for i in range(ncols)], 'formats': ncols * [A.dtype]}\n",
        "    return np.in1d(A.view(dtype), B.view(dtype))  # boolean return\n",
        "\n",
        "class Sparse_DAE:\n",
        "    def __init__(self, dimensions, activations, epsilon=20):\n",
        "        self.n_layers = len(dimensions)\n",
        "        self.loss = None\n",
        "        self.learning_rate = None\n",
        "        self.momentum = None\n",
        "        self.weight_decay = None\n",
        "        self.epsilon = epsilon  # control the sparsity level as discussed in the paper\n",
        "        self.zeta = None  # the fraction of the weights removed\n",
        "        self.droprate = 0  # dropout rate\n",
        "        self.dimensions = dimensions\n",
        "\n",
        "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
        "        self.w = {}\n",
        "        self.b = {}\n",
        "        self.pdw = {}\n",
        "        self.pdd = {}\n",
        "\n",
        "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
        "        self.activations = {}\n",
        "        for i in range(len(dimensions) - 1):\n",
        "            if (i<len(dimensions) - 2):\n",
        "                self.w[i + 1] = createSparseWeights(self.epsilon, dimensions[i],\n",
        "                                                dimensions[i + 1])  # create sparse weight matrices\n",
        "            else:\n",
        "                self.w[i + 1] = createSparseWeights(self.epsilon, dimensions[i],\n",
        "                                                dimensions[i + 1])  # create sparse weight matrices\n",
        "\n",
        "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
        "            self.activations[i + 2] = activations[i]\n",
        "\n",
        "\n",
        "    def _feed_forward(self, x, drop=False):\n",
        "        # w(x) + b\n",
        "        z = {}\n",
        "        # activations: f(z)\n",
        "        a = {1: x}  # First layer has no activations as input. The input x is the input.\n",
        "\n",
        "        for i in range(1, self.n_layers):\n",
        "            z[i + 1] = a[i] @ self.w[i] + self.b[i]\n",
        "            if (drop == False):\n",
        "                if (i > 1):\n",
        "                    z[i + 1] = z[i + 1] * (1 - self.droprate)\n",
        "            a[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
        "            if (drop):\n",
        "                if (i < self.n_layers - 1):\n",
        "                    dropMask = np.random.rand(a[i + 1].shape[0], a[i + 1].shape[1])\n",
        "                    dropMask[dropMask >= self.droprate] = 1\n",
        "                    dropMask[dropMask < self.droprate] = 0\n",
        "                    a[i + 1] = dropMask * a[i + 1]\n",
        "\n",
        "        return z, a\n",
        "\n",
        "    def _back_prop(self, z, a, adjusted_X_train):\n",
        "        delta = self.loss.delta(adjusted_X_train, a[self.n_layers])\n",
        "        dw = coo_matrix(self.w[self.n_layers - 1])\n",
        "\n",
        "        # compute backpropagation updates\n",
        "        #sparseoperations.backpropagation_updates_Cython(a[self.n_layers - 1],delta,dw.row,dw.col,dw.data)# If you have problems with Cython please use the backpropagation_updates_Numpy method by uncommenting the line below and commenting the one above. Please note that the running time will be much higher\n",
        "        # If you have problems with Cython please use the backpropagation_updates_Numpy method by uncommenting the line below and commenting the one above. Please note that the running time will be much higher\n",
        "        backpropagation_updates_Numpy(a[self.n_layers - 1], delta, dw.row, dw.col, dw.data)\n",
        "\n",
        "        update_params = {\n",
        "            self.n_layers - 1: (dw.tocsr(), delta)\n",
        "        }\n",
        "        for i in reversed(range(2, self.n_layers)):\n",
        "            delta = (delta @ self.w[i].transpose()) * self.activations[i].prime(z[i])\n",
        "            dw = coo_matrix(self.w[i - 1])\n",
        "\n",
        "            # compute backpropagation updates\n",
        "            #sparseoperations.backpropagation_updates_Cython(a[i - 1], delta, dw.row, dw.col, dw.data)# If you have problems with Cython please use the backpropagation_updates_Numpy method by uncommenting the line below and commenting the one above. Please note that the running time will be much higher\n",
        "            # If you have problems with Cython please use the backpropagation_updates_Numpy method by uncommenting the line below and commenting the one above. Please note that the running time will be much higher\n",
        "            backpropagation_updates_Numpy(a[i - 1], delta, dw.row, dw.col, dw.data)\n",
        "\n",
        "            update_params[i - 1] = (dw.tocsr(), delta)\n",
        "            for k, v in update_params.items():\n",
        "                self._update_w_b(k, v[0], v[1])\n",
        "\n",
        "    def _update_w_b(self, index, dw, delta):\n",
        "        \"\"\"\n",
        "        Update weights and biases.\n",
        "        :param index: (int) Number of the layer\n",
        "        :param dw: (array) Partial derivatives\n",
        "        :param delta: (array) Delta error.\n",
        "        \"\"\"\n",
        "        # perform the update with momentum\n",
        "        if (index not in self.pdw):\n",
        "            self.pdw[index] = -self.learning_rate * dw\n",
        "            self.pdd[index] = - self.learning_rate * np.mean(delta, 0)\n",
        "        else:\n",
        "            self.pdw[index] = self.momentum * self.pdw[index] - self.learning_rate * dw\n",
        "            self.pdd[index] = self.momentum * self.pdd[index] - self.learning_rate * np.mean(delta, 0)\n",
        "\n",
        "        self.w[index] += self.pdw[index] - self.weight_decay * self.w[index]\n",
        "        self.b[index] += self.pdd[index] - self.weight_decay * self.b[index]\n",
        "\n",
        "    def fit(self, x, y_true, x_test, y_test, loss, epochs, batch_size, learning_rate=1e-3, momentum=0.9,\n",
        "            weight_decay=0.00001, zeta=0.2, dropoutrate=0, testing=True, save_filename=\"\", noise_factor = 0.2):\n",
        "\n",
        "        if not x.shape[0] == y_true.shape[0]:\n",
        "            raise ValueError(\"Length of x and y arrays don't match\")\n",
        "\n",
        "        # Initiate the loss object with the final activation function\n",
        "        self.loss = loss(self.activations[self.n_layers])\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.zeta = zeta\n",
        "        self.droprate = dropoutrate\n",
        "        self.save_filename=save_filename\n",
        "        self.inputLayerConnections = []\n",
        "        self.inputLayerConnections.append(self.getCoreInputConnections())\n",
        "        np.savez_compressed(self.save_filename + \"_input_connections.npz\",\n",
        "                        inputLayerConnections=self.inputLayerConnections)\n",
        "\n",
        "\n",
        "\n",
        "        metrics = np.zeros((epochs, 2))\n",
        "        mean = 0; stddev = 1\n",
        "        noise = noise_factor * np.random.normal(mean, stddev, (x.shape[0], x.shape[1]))\n",
        "        x_noisy = x + noise\n",
        "        seed = np.arange(x.shape[0])\n",
        "\n",
        "        for i in range(epochs):\n",
        "            print(\"----------------------------------------------------\")\n",
        "            print(\"epoch \", i)\n",
        "\n",
        "\n",
        "            # Shuffle the data\n",
        "            np.random.shuffle(seed)\n",
        "            x_noisy_ = x_noisy[seed]\n",
        "            x_ = x[seed]\n",
        "            y_ = y_true[seed]\n",
        "            # training\n",
        "            t1 = datetime.datetime.now()\n",
        "            for j in range(x.shape[0] // batch_size):\n",
        "                k = j * batch_size\n",
        "                l = (j + 1) * batch_size\n",
        "                z, a = self._feed_forward(x_noisy_[k:l], True)\n",
        "                self._back_prop(z, a, x_[k:l])\n",
        "            t2 = datetime.datetime.now()\n",
        "            print(\"Training time: \", t2 - t1)\n",
        "\n",
        "            if (testing):\n",
        "                activations_hidden_test, activations_output_test  = self.predict(x_test)\n",
        "                activations_hidden_train, activations_output_train = self.predict(x)\n",
        "                loss_train = self.loss.loss(x, activations_output_train)\n",
        "                metrics[i, 0] = loss_train\n",
        "                loss_test = self.loss.loss(x_test, activations_output_test)\n",
        "                metrics[i, 1] = loss_test\n",
        "                print(\"Loss train: \", loss_train, \"; Loss test: \", loss_test)\n",
        "                plt_loss(metrics[:i+1, 0], metrics[:i+1, 1], self.save_filename)\n",
        "\n",
        "            if (i < epochs - 1):\n",
        "                # do not change connectivity pattern after the last epoch\n",
        "                # self.weightsEvolution_I() #this implementation is more didactic, but slow.\n",
        "                w1 , w2= self.weightsEvolution_II()  # this implementation has the same behaviour as the one above, but it is much faster.\n",
        "\n",
        "\n",
        "\n",
        "            if (self.save_filename != \"\"):\n",
        "                np.savetxt(self.save_filename+\"/metrics.txt\", metrics)\n",
        "        scipy.sparse.save_npz(self.save_filename+\"w1\", w1)\n",
        "        scipy.sparse.save_npz(self.save_filename+\"w2\", w2)\n",
        "        return metrics\n",
        "    def getCoreInputConnections(self):\n",
        "        values = np.sort(self.w[1].data)\n",
        "        firstZeroPos = find_first_pos(values, 0)\n",
        "        lastZeroPos = find_last_pos(values, 0)\n",
        "\n",
        "        largestNegative = values[int((1 - self.zeta) * firstZeroPos)]\n",
        "        smallestPositive = values[\n",
        "            int(min(values.shape[0] - 1, lastZeroPos + self.zeta * (values.shape[0] - lastZeroPos)))]\n",
        "\n",
        "        wlil = self.w[1].tolil()\n",
        "        wdok = dok_matrix((self.dimensions[0], self.dimensions[1]), dtype=\"float64\")\n",
        "\n",
        "        # remove the weights closest to zero\n",
        "        keepConnections = 0\n",
        "        for ik, (row, data) in enumerate(zip(wlil.rows, wlil.data)):\n",
        "            for jk, val in zip(row, data):\n",
        "                if ((val < largestNegative) or (val > smallestPositive)):\n",
        "                    wdok[ik, jk] = val\n",
        "                    keepConnections += 1\n",
        "        return wdok.tocsr().getnnz(axis=1)\n",
        "\n",
        "    def weightsEvolution_I(self):\n",
        "        # this represents the core of the SET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
        "        for i in range(1, self.n_layers):\n",
        "\n",
        "            values = np.sort(self.w[i].data)\n",
        "            firstZeroPos = find_first_pos(values, 0)\n",
        "            lastZeroPos = find_last_pos(values, 0)\n",
        "\n",
        "            largestNegative = values[int((1 - self.zeta) * firstZeroPos)]\n",
        "            smallestPositive = values[\n",
        "                int(min(values.shape[0] - 1, lastZeroPos + self.zeta * (values.shape[0] - lastZeroPos)))]\n",
        "\n",
        "            wlil = self.w[i].tolil()\n",
        "            pdwlil = self.pdw[i].tolil()\n",
        "            wdok = dok_matrix((self.dimensions[i - 1], self.dimensions[i]), dtype=\"float64\")\n",
        "            pdwdok = dok_matrix((self.dimensions[i - 1], self.dimensions[i]), dtype=\"float64\")\n",
        "\n",
        "            # remove the weights closest to zero\n",
        "            keepConnections = 0\n",
        "            for ik, (row, data) in enumerate(zip(wlil.rows, wlil.data)):\n",
        "                for jk, val in zip(row, data):\n",
        "                    if ((val < largestNegative) or (val > smallestPositive)):\n",
        "                        wdok[ik, jk] = val\n",
        "                        pdwdok[ik, jk] = pdwlil[ik, jk]\n",
        "                        keepConnections += 1\n",
        "\n",
        "            # add new random connections\n",
        "            for kk in range(self.w[i].data.shape[0] - keepConnections):\n",
        "                ik = np.random.randint(0, self.dimensions[i - 1])\n",
        "                jk = np.random.randint(0, self.dimensions[i])\n",
        "                while (wdok[ik, jk] != 0):\n",
        "                    ik = np.random.randint(0, self.dimensions[i - 1])\n",
        "                    jk = np.random.randint(0, self.dimensions[i])\n",
        "                wdok[ik, jk] = np.random.randn() / 10\n",
        "                pdwdok[ik, jk] = 0\n",
        "\n",
        "            self.pdw[i] = pdwdok.tocsr()\n",
        "            self.w[i] = wdok.tocsr()\n",
        "\n",
        "    def weightsEvolution_II(self):\n",
        "        # this represents the core of the SET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
        "        #evolve all layers, except the one from the last hidden layer to the output layer\n",
        "        for i in range(1, self.n_layers):\n",
        "            # uncomment line below to stop evolution of dense weights more than 80% non-zeros\n",
        "            #if(self.w[i].count_nonzero()/(self.w[i].get_shape()[0]*self.w[i].get_shape()[1]) < 0.8):\n",
        "                t_ev_1 = datetime.datetime.now()\n",
        "                # converting to COO form\n",
        "                wcoo = self.w[i].tocoo()\n",
        "                valsW = wcoo.data\n",
        "                rowsW = wcoo.row\n",
        "                colsW = wcoo.col\n",
        "\n",
        "                pdcoo = self.pdw[i].tocoo()\n",
        "                valsPD = pdcoo.data\n",
        "                rowsPD = pdcoo.row\n",
        "                colsPD = pdcoo.col\n",
        "                # print(\"Number of non zeros in W and PD matrix before evolution in layer\",i,[np.size(valsW), np.size(valsPD)])\n",
        "                values = np.sort(self.w[i].data)\n",
        "                firstZeroPos = find_first_pos(values, 0)\n",
        "                lastZeroPos = find_last_pos(values, 0)\n",
        "\n",
        "                largestNegative = values[int((1 - self.zeta) * firstZeroPos)]\n",
        "                smallestPositive = values[\n",
        "                    int(min(values.shape[0] - 1, lastZeroPos + self.zeta * (values.shape[0] - lastZeroPos)))]\n",
        "\n",
        "                # remove the weights (W) closest to zero and modify PD as well\n",
        "                valsWNew = valsW[(valsW > smallestPositive) | (valsW < largestNegative)]\n",
        "                rowsWNew = rowsW[(valsW > smallestPositive) | (valsW < largestNegative)]\n",
        "                colsWNew = colsW[(valsW > smallestPositive) | (valsW < largestNegative)]\n",
        "\n",
        "                newWRowColIndex = np.stack((rowsWNew, colsWNew), axis=-1)\n",
        "                oldPDRowColIndex = np.stack((rowsPD, colsPD), axis=-1)\n",
        "\n",
        "                newPDRowColIndexFlag = array_intersect(oldPDRowColIndex, newWRowColIndex)  # careful about order\n",
        "\n",
        "                valsPDNew = valsPD[newPDRowColIndexFlag]\n",
        "                rowsPDNew = rowsPD[newPDRowColIndexFlag]\n",
        "                colsPDNew = colsPD[newPDRowColIndexFlag]\n",
        "\n",
        "                self.pdw[i] = coo_matrix((valsPDNew, (rowsPDNew, colsPDNew)),\n",
        "                                         (self.dimensions[i - 1], self.dimensions[i])).tocsr()\n",
        "\n",
        "                if(i==1):\n",
        "                    w1 = coo_matrix((valsWNew, (rowsWNew, colsWNew)),\n",
        "                                       (self.dimensions[i - 1], self.dimensions[i])).tocsr()\n",
        "                    self.inputLayerConnections.append(coo_matrix((valsWNew, (rowsWNew, colsWNew)),\n",
        "                                       (self.dimensions[i - 1], self.dimensions[i])).getnnz(axis=1))\n",
        "                    np.savez_compressed(self.save_filename + \"_input_connections.npz\",\n",
        "                                        inputLayerConnections=self.inputLayerConnections)\n",
        "                else:\n",
        "                    w2 = coo_matrix((valsWNew, (rowsWNew, colsWNew)),\n",
        "                                       (self.dimensions[i - 1], self.dimensions[i])).tocsr()\n",
        "\n",
        "\n",
        "                # add new random connections\n",
        "                keepConnections = np.size(rowsWNew)\n",
        "                lengthRandom = valsW.shape[0] - keepConnections\n",
        "                randomVals = np.random.randn(lengthRandom) / 10\n",
        "                zeroVals = 0 * randomVals  # explicit zeros\n",
        "\n",
        "                # adding  (wdok[ik,jk]!=0): condition\n",
        "                while (lengthRandom > 0):\n",
        "                    ik = np.random.randint(0, self.dimensions[i - 1], size=lengthRandom, dtype='int32')\n",
        "                    jk = np.random.randint(0, self.dimensions[i], size=lengthRandom, dtype='int32')\n",
        "\n",
        "                    randomWRowColIndex = np.stack((ik, jk), axis=-1)\n",
        "                    randomWRowColIndex = np.unique(randomWRowColIndex, axis=0)  # removing duplicates in new rows&cols\n",
        "                    oldWRowColIndex = np.stack((rowsWNew, colsWNew), axis=-1)\n",
        "\n",
        "                    uniqueFlag = ~array_intersect(randomWRowColIndex, oldWRowColIndex)  # careful about order & tilda\n",
        "\n",
        "                    ikNew = randomWRowColIndex[uniqueFlag][:, 0]\n",
        "                    jkNew = randomWRowColIndex[uniqueFlag][:, 1]\n",
        "                    # be careful - row size and col size needs to be verified\n",
        "                    rowsWNew = np.append(rowsWNew, ikNew)\n",
        "                    colsWNew = np.append(colsWNew, jkNew)\n",
        "\n",
        "                    lengthRandom = valsW.shape[0] - np.size(rowsWNew)  # this will constantly reduce lengthRandom\n",
        "\n",
        "                # adding all the values along with corresponding row and column indices\n",
        "                valsWNew = np.append(valsWNew, randomVals)\n",
        "                # valsPDNew=np.append(valsPDNew, zeroVals)\n",
        "                if (valsWNew.shape[0] != rowsWNew.shape[0]):\n",
        "                    print(\"not good\")\n",
        "                self.w[i] = coo_matrix((valsWNew, (rowsWNew, colsWNew)),\n",
        "                                       (self.dimensions[i - 1], self.dimensions[i])).tocsr()\n",
        "\n",
        "                # print(\"Number of non zeros in W and PD matrix after evolution in layer\",i,[(self.w[i].data.shape[0]), (self.pdw[i].data.shape[0])])\n",
        "\n",
        "                t_ev_2 = datetime.datetime.now()\n",
        "                print(\"Weights evolution time for layer\",i,\"is\", t_ev_2 - t_ev_1)\n",
        "        return w1 , w2\n",
        "\n",
        "    def predict(self, x):\n",
        "        _, a = self._feed_forward(x)\n",
        "        a_hidden = a[self.n_layers-1]\n",
        "        a_output = a[self.n_layers]\n",
        "        del a\n",
        "        return a_hidden, a_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTXzrr1Zd9qU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5Vv8_8fIUx"
      },
      "source": [
        "# **Utils.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l3FP74Vgfuz"
      },
      "source": [
        "This code is an adjustment of the existing code that stems from: Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEe0NpB1d9wx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(name):\n",
        "\n",
        "    if name == \"coil20\":\n",
        "        mat = scipy.io.loadmat(\"/content/gdrive/MyDrive/datasets/Coil20.mat\")\n",
        "        print(mat['images'])\n",
        "        X = mat['images']\n",
        "        y = mat['labels']\n",
        "        y = np.array([int(label[3:]) for label in y])\n",
        "        print(\"labels\", y)\n",
        "\n",
        "        num_samples, height, width = X.shape\n",
        "        X = X.reshape((num_samples, height * width))\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        print(\"type X_train\", type(X_train))\n",
        "        print(\"type y_train\", type(y_train))\n",
        "        print(\"first element X_train\", X_train[0])\n",
        "        print(\"first element y_train\", y_train[0])\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "    elif name == \"MNIST\":\n",
        "        filename = f\"/content/gdrive/MyDrive/datasets/MNIST_{target_gini}.npz\"\n",
        "        data = np.load(filename)\n",
        "\n",
        "        X_train = data['X_train']\n",
        "        y_train = data['y_train']\n",
        "        X_test = data['X_test']\n",
        "        y_test = data['y_test']\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def check_path(filename):\n",
        "    import os\n",
        "    if not os.path.exists(os.path.dirname(filename)):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(filename))\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSX6in8KgSCe"
      },
      "source": [
        "# **Train sparse DAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sr09rbnoQua"
      },
      "source": [
        "The given code is an implementation of a Sparse Denoising Autoencoder (DAE). The DAE is a type of neural network that is trained to reconstruct its input data by learning a compressed representation in an intermediate hidden layer.\n",
        "\n",
        "This code is an adjustment of the existing code that stems from: Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560.\n",
        "\n",
        "and\n",
        "\n",
        "Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., & Liotta, A. (2018). Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1), 2383."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1CfQRu9gXZx"
      },
      "outputs": [],
      "source": [
        "def train_sparse_DAE(dataset, epoch):\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Train Sparse DAE')\n",
        "  #parser.add_argument('--dataset_name', type=str, required=True, help='dataset_name')\n",
        "  #parser.add_argument(\"--epoch\", help=\"epoch\", type=int)\n",
        "  #args = parser.parse_args()\n",
        "  #dataset_name = args.dataset_name\n",
        "\n",
        "\n",
        "  strtime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "  noise_factor = 0.2\n",
        "  weightDecay = 0.00001\n",
        "  noHiddenNeuronsLayer=1000\n",
        "  batchSize=100\n",
        "  dropoutRate=0.2\n",
        "  learningRate=0.01\n",
        "  momentum=0.9\n",
        "  epsilon = 17 #a given sparsity level\n",
        "  zeta = 0.2\n",
        "  adjusted_X_train, adjusted_y_train, adjusted_X_test, adjusted_y_test = load_data(dataset_name)\n",
        "  adjusted_y_train = np.array(adjusted_y_train)\n",
        "  unique_labels = np.unique(adjusted_y_train)\n",
        "  label_counts = np.array([np.count_nonzero(adjusted_y_train == label) for label in unique_labels])\n",
        "  relative_distribution = label_counts / len(adjusted_y_train)\n",
        "  print(\"relative_distribution\", relative_distribution)\n",
        "  print(\"The gini-coefficient of the adjusted trainingsdata =\")\n",
        "  print(calculate_gini(relative_distribution))\n",
        "  print(\"The amount of cases in the trainingsdata =\")\n",
        "  print(adjusted_y_train.shape)\n",
        "\n",
        "#in range 5 so the mean and standard deviation can be taken\n",
        "  for i in range(5):\n",
        "    filename = \"/content/gdrive/MyDrive/results/\" + dataset_name+\"/\"+str(strtime)+\"_target_gini_\"+str(target_gini)+\"_epsilon_\" + str(epsilon) + \"_zeta_\" + str(zeta) + \"/run \" +str(i)+ \"/\"\n",
        "    check_path(filename)\n",
        "    print(\"*******************************************************************************\")\n",
        "    print(\"Dataset = \", dataset_name)\n",
        "    print(\"epsilon = \", epsilon)\n",
        "    print(\"zeta = \", zeta)\n",
        "    print(\"3. Imbalanced Dataset is made\")\n",
        "    noTrainingSamples = adjusted_X_train.shape[0]\n",
        "\n",
        "\n",
        "    start = time.time()\n",
        "    set_dae = Sparse_DAE((adjusted_X_train.shape[1], noHiddenNeuronsLayer, adjusted_X_train.shape[1]), (Sigmoid, Linear), epsilon=epsilon)\n",
        "\n",
        "    set_dae.fit(adjusted_X_train, adjusted_y_train.ravel(), adjusted_X_test, adjusted_y_test.ravel(), loss=MSE,\n",
        "                  epochs=epoch, batch_size=batchSize, learning_rate=learningRate,\n",
        "                  momentum=momentum, weight_decay=weightDecay, zeta=zeta, dropoutrate=dropoutRate,\n",
        "                  testing=False, save_filename=filename, noise_factor = noise_factor)\n",
        "    end = time.time()\n",
        "    print(\"running time QS_\",epoch,\"= \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikvt7q9sn0j7"
      },
      "source": [
        "# **Training (Sparse DAE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8y_jmuGKb6H"
      },
      "source": [
        "The sparse DAE is used for unsupervised feature learning. Unsupervised feature learning is a machine learning technique where a model learns to extract useful features or representations from unlabeled data without the need for explicit supervision or labeled examples. Autoencoders are neural network models that learn to reconstruct the input data from a compressed representation called the \"latent space.\" After training the SDAE, the code 'QuickSelection' applies feature selection on the learned features to select a subset of the most relevant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Ec2xMOiimf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f2c3d9-85d7-4441-d1f3-4bf06997bf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relative_distribution [0.70135747 0.0331825  0.0331825  0.0331825  0.0331825  0.0331825\n",
            " 0.0331825  0.0331825  0.0331825  0.0331825 ]\n",
            "The gini-coefficient of the adjusted trainingsdata =\n",
            "0.6013574660633484\n",
            "The amount of cases in the trainingsdata =\n",
            "(6630,)\n",
            "*******************************************************************************\n",
            "Dataset =  MNIST\n",
            "epsilon =  13\n",
            "zeta =  0.2\n",
            "3. Imbalanced Dataset is made\n",
            "Create sparse matrix with  22823  connections and  2.9110969387755103 % density level\n",
            "Create sparse matrix with  22882  connections and  2.9186224489795918 % density level\n",
            "----------------------------------------------------\n",
            "epoch  0\n",
            "Training time:  0:02:38.574642\n",
            "Weights evolution time for layer 1 is 0:00:00.083262\n",
            "Weights evolution time for layer 2 is 0:00:00.079001\n",
            "----------------------------------------------------\n",
            "epoch  1\n",
            "Training time:  0:02:36.434393\n",
            "Weights evolution time for layer 1 is 0:00:00.077709\n",
            "Weights evolution time for layer 2 is 0:00:00.073649\n",
            "----------------------------------------------------\n",
            "epoch  2\n",
            "Training time:  0:02:36.952503\n",
            "Weights evolution time for layer 1 is 0:00:00.063209\n",
            "Weights evolution time for layer 2 is 0:00:00.092141\n",
            "----------------------------------------------------\n",
            "epoch  3\n",
            "Training time:  0:02:35.204915\n",
            "Weights evolution time for layer 1 is 0:00:00.084224\n",
            "Weights evolution time for layer 2 is 0:00:00.079893\n",
            "----------------------------------------------------\n",
            "epoch  4\n",
            "Training time:  0:02:36.193213\n",
            "Weights evolution time for layer 1 is 0:00:00.079318\n",
            "Weights evolution time for layer 2 is 0:00:00.073165\n",
            "----------------------------------------------------\n",
            "epoch  5\n",
            "Training time:  0:02:35.927903\n",
            "Weights evolution time for layer 1 is 0:00:00.061892\n",
            "Weights evolution time for layer 2 is 0:00:00.071860\n",
            "----------------------------------------------------\n",
            "epoch  6\n",
            "Training time:  0:02:34.507025\n",
            "Weights evolution time for layer 1 is 0:00:00.081142\n",
            "Weights evolution time for layer 2 is 0:00:00.072542\n",
            "----------------------------------------------------\n",
            "epoch  7\n",
            "Training time:  0:02:35.497396\n",
            "Weights evolution time for layer 1 is 0:00:00.079561\n",
            "Weights evolution time for layer 2 is 0:00:00.071503\n",
            "----------------------------------------------------\n",
            "epoch  8\n",
            "Training time:  0:02:36.178440\n",
            "Weights evolution time for layer 1 is 0:00:00.083509\n",
            "Weights evolution time for layer 2 is 0:00:00.078096\n",
            "----------------------------------------------------\n",
            "epoch  9\n",
            "Training time:  0:02:35.324031\n",
            "running time QS_ 10 =  1563.2362220287323\n",
            "*******************************************************************************\n",
            "Dataset =  MNIST\n",
            "epsilon =  13\n",
            "zeta =  0.2\n",
            "3. Imbalanced Dataset is made\n",
            "Create sparse matrix with  22861  connections and  2.9159438775510202 % density level\n",
            "Create sparse matrix with  22866  connections and  2.9165816326530614 % density level\n",
            "----------------------------------------------------\n",
            "epoch  0\n",
            "Training time:  0:02:37.186320\n",
            "Weights evolution time for layer 1 is 0:00:00.077291\n",
            "Weights evolution time for layer 2 is 0:00:00.080515\n",
            "----------------------------------------------------\n",
            "epoch  1\n",
            "Training time:  0:02:35.764178\n",
            "Weights evolution time for layer 1 is 0:00:00.097487\n",
            "Weights evolution time for layer 2 is 0:00:00.073159\n",
            "----------------------------------------------------\n",
            "epoch  2\n",
            "Training time:  0:02:36.411758\n",
            "Weights evolution time for layer 1 is 0:00:00.086286\n",
            "Weights evolution time for layer 2 is 0:00:00.079512\n",
            "----------------------------------------------------\n",
            "epoch  3\n",
            "Training time:  0:02:37.724767\n",
            "Weights evolution time for layer 1 is 0:00:00.081588\n",
            "Weights evolution time for layer 2 is 0:00:00.074481\n",
            "----------------------------------------------------\n",
            "epoch  4\n",
            "Training time:  0:02:36.256543\n",
            "Weights evolution time for layer 1 is 0:00:00.076513\n",
            "Weights evolution time for layer 2 is 0:00:00.073672\n",
            "----------------------------------------------------\n",
            "epoch  5\n",
            "Training time:  0:02:36.315351\n",
            "Weights evolution time for layer 1 is 0:00:00.080366\n",
            "Weights evolution time for layer 2 is 0:00:00.072902\n",
            "----------------------------------------------------\n",
            "epoch  6\n",
            "Training time:  0:02:34.430534\n",
            "Weights evolution time for layer 1 is 0:00:00.082467\n",
            "Weights evolution time for layer 2 is 0:00:00.074097\n",
            "----------------------------------------------------\n",
            "epoch  7\n",
            "Training time:  0:02:36.498433\n",
            "Weights evolution time for layer 1 is 0:00:00.086375\n",
            "Weights evolution time for layer 2 is 0:00:00.073791\n",
            "----------------------------------------------------\n",
            "epoch  8\n",
            "Training time:  0:02:35.483425\n",
            "Weights evolution time for layer 1 is 0:00:00.085094\n",
            "Weights evolution time for layer 2 is 0:00:00.074409\n",
            "----------------------------------------------------\n",
            "epoch  9\n",
            "Training time:  0:02:37.531024\n",
            "running time QS_ 10 =  1566.0039715766907\n",
            "*******************************************************************************\n",
            "Dataset =  MNIST\n",
            "epsilon =  13\n",
            "zeta =  0.2\n",
            "3. Imbalanced Dataset is made\n",
            "Create sparse matrix with  22867  connections and  2.9167091836734693 % density level\n",
            "Create sparse matrix with  22825  connections and  2.9113520408163263 % density level\n",
            "----------------------------------------------------\n",
            "epoch  0\n",
            "Training time:  0:02:36.502367\n",
            "Weights evolution time for layer 1 is 0:00:00.081884\n",
            "Weights evolution time for layer 2 is 0:00:00.082380\n",
            "----------------------------------------------------\n",
            "epoch  1\n",
            "Training time:  0:02:35.407652\n",
            "Weights evolution time for layer 1 is 0:00:00.079895\n",
            "Weights evolution time for layer 2 is 0:00:00.072908\n",
            "----------------------------------------------------\n",
            "epoch  2\n",
            "Training time:  0:02:36.676750\n",
            "Weights evolution time for layer 1 is 0:00:00.089562\n",
            "Weights evolution time for layer 2 is 0:00:00.077094\n",
            "----------------------------------------------------\n",
            "epoch  3\n",
            "Training time:  0:02:34.660652\n",
            "Weights evolution time for layer 1 is 0:00:00.089854\n",
            "Weights evolution time for layer 2 is 0:00:00.079237\n",
            "----------------------------------------------------\n",
            "epoch  4\n",
            "Training time:  0:02:37.114310\n",
            "Weights evolution time for layer 1 is 0:00:00.081241\n",
            "Weights evolution time for layer 2 is 0:00:00.076721\n",
            "----------------------------------------------------\n",
            "epoch  5\n",
            "Training time:  0:02:38.095784\n",
            "Weights evolution time for layer 1 is 0:00:00.110850\n",
            "Weights evolution time for layer 2 is 0:00:00.143702\n",
            "----------------------------------------------------\n",
            "epoch  6\n",
            "Training time:  0:02:37.941080\n",
            "Weights evolution time for layer 1 is 0:00:00.139168\n",
            "Weights evolution time for layer 2 is 0:00:00.167133\n",
            "----------------------------------------------------\n",
            "epoch  7\n",
            "Training time:  0:02:36.523841\n",
            "Weights evolution time for layer 1 is 0:00:00.146343\n",
            "Weights evolution time for layer 2 is 0:00:00.135553\n",
            "----------------------------------------------------\n",
            "epoch  8\n",
            "Training time:  0:02:34.660759\n",
            "Weights evolution time for layer 1 is 0:00:00.142098\n",
            "Weights evolution time for layer 2 is 0:00:00.137650\n",
            "----------------------------------------------------\n",
            "epoch  9\n",
            "Training time:  0:02:35.539380\n",
            "running time QS_ 10 =  1565.9894030094147\n",
            "*******************************************************************************\n",
            "Dataset =  MNIST\n",
            "epsilon =  13\n",
            "zeta =  0.2\n",
            "3. Imbalanced Dataset is made\n",
            "Create sparse matrix with  22867  connections and  2.9167091836734693 % density level\n",
            "Create sparse matrix with  22822  connections and  2.910969387755102 % density level\n",
            "----------------------------------------------------\n",
            "epoch  0\n",
            "Training time:  0:02:35.848049\n",
            "Weights evolution time for layer 1 is 0:00:00.136686\n",
            "Weights evolution time for layer 2 is 0:00:00.161825\n",
            "----------------------------------------------------\n",
            "epoch  1\n",
            "Training time:  0:02:37.532208\n",
            "Weights evolution time for layer 1 is 0:00:00.063845\n",
            "Weights evolution time for layer 2 is 0:00:00.075050\n",
            "----------------------------------------------------\n",
            "epoch  2\n",
            "Training time:  0:02:36.471431\n",
            "Weights evolution time for layer 1 is 0:00:00.146038\n",
            "Weights evolution time for layer 2 is 0:00:00.077340\n",
            "----------------------------------------------------\n",
            "epoch  3\n",
            "Training time:  0:02:37.102650\n",
            "Weights evolution time for layer 1 is 0:00:00.079571\n",
            "Weights evolution time for layer 2 is 0:00:00.072518\n",
            "----------------------------------------------------\n",
            "epoch  4\n",
            "Training time:  0:02:35.429243\n",
            "Weights evolution time for layer 1 is 0:00:00.080065\n",
            "Weights evolution time for layer 2 is 0:00:00.084116\n",
            "----------------------------------------------------\n",
            "epoch  5\n",
            "Training time:  0:02:37.207354\n",
            "Weights evolution time for layer 1 is 0:00:00.082419\n",
            "Weights evolution time for layer 2 is 0:00:00.072854\n",
            "----------------------------------------------------\n",
            "epoch  6\n",
            "Training time:  0:02:37.313576\n",
            "Weights evolution time for layer 1 is 0:00:00.102585\n",
            "Weights evolution time for layer 2 is 0:00:00.082074\n",
            "----------------------------------------------------\n",
            "epoch  7\n",
            "Training time:  0:02:38.778990\n",
            "Weights evolution time for layer 1 is 0:00:00.102902\n",
            "Weights evolution time for layer 2 is 0:00:00.073337\n",
            "----------------------------------------------------\n",
            "epoch  8\n",
            "Training time:  0:02:36.210425\n",
            "Weights evolution time for layer 1 is 0:00:00.108313\n",
            "Weights evolution time for layer 2 is 0:00:00.077655\n",
            "----------------------------------------------------\n",
            "epoch  9\n",
            "Training time:  0:02:37.111742\n",
            "running time QS_ 10 =  1572.0193259716034\n",
            "*******************************************************************************\n",
            "Dataset =  MNIST\n",
            "epsilon =  13\n",
            "zeta =  0.2\n",
            "3. Imbalanced Dataset is made\n",
            "Create sparse matrix with  22837  connections and  2.9128826530612244 % density level\n",
            "Create sparse matrix with  22831  connections and  2.9121173469387753 % density level\n",
            "----------------------------------------------------\n",
            "epoch  0\n",
            "Training time:  0:02:36.943982\n",
            "Weights evolution time for layer 1 is 0:00:00.075847\n",
            "Weights evolution time for layer 2 is 0:00:00.074215\n",
            "----------------------------------------------------\n",
            "epoch  1\n",
            "Training time:  0:02:35.394343\n",
            "Weights evolution time for layer 1 is 0:00:00.082368\n",
            "Weights evolution time for layer 2 is 0:00:00.075514\n",
            "----------------------------------------------------\n",
            "epoch  2\n",
            "Training time:  0:02:36.573234\n",
            "Weights evolution time for layer 1 is 0:00:00.080103\n",
            "Weights evolution time for layer 2 is 0:00:00.073239\n",
            "----------------------------------------------------\n",
            "epoch  3\n",
            "Training time:  0:02:37.212313\n",
            "Weights evolution time for layer 1 is 0:00:00.078815\n",
            "Weights evolution time for layer 2 is 0:00:00.075541\n",
            "----------------------------------------------------\n",
            "epoch  4\n",
            "Training time:  0:02:36.911159\n",
            "Weights evolution time for layer 1 is 0:00:00.104779\n",
            "Weights evolution time for layer 2 is 0:00:00.058734\n",
            "----------------------------------------------------\n",
            "epoch  5\n",
            "Training time:  0:02:39.953496\n",
            "Weights evolution time for layer 1 is 0:00:00.150688\n",
            "Weights evolution time for layer 2 is 0:00:00.133066\n",
            "----------------------------------------------------\n",
            "epoch  6\n",
            "Training time:  0:02:38.163110\n",
            "Weights evolution time for layer 1 is 0:00:00.084554\n",
            "Weights evolution time for layer 2 is 0:00:00.094035\n",
            "----------------------------------------------------\n",
            "epoch  7\n",
            "Training time:  0:02:37.459355\n",
            "Weights evolution time for layer 1 is 0:00:00.081067\n",
            "Weights evolution time for layer 2 is 0:00:00.073683\n",
            "----------------------------------------------------\n",
            "epoch  8\n",
            "Training time:  0:02:36.435083\n",
            "Weights evolution time for layer 1 is 0:00:00.138077\n",
            "Weights evolution time for layer 2 is 0:00:00.132356\n",
            "----------------------------------------------------\n",
            "epoch  9\n",
            "Training time:  0:02:37.831424\n",
            "running time QS_ 10 =  1575.4656903743744\n"
          ]
        }
      ],
      "source": [
        "train_sparse_DAE(dataset_name, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0l320stfWLv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "92d161dd-af24-4bc1-8167-3899db735442"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-805b61d09d89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgini2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgini_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_gini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madjusted_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimbalanced_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
          ]
        }
      ],
      "source": [
        "num_categories = len(set(Y_train))\n",
        "gini2, distribution2 = gini_algorithm(target_gini, num_categories)\n",
        "adjusted_X_train, adjusted_y_train, adjusted_X_test, adjusted_y_test = imbalanced_data(X_train, Y_train, X_test, Y_test, distribution2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtcAzVRYHMMn"
      },
      "outputs": [],
      "source": [
        "adjusted_y_train = np.array(adjusted_y_train)\n",
        "unique_labels = np.unique(adjusted_y_train)\n",
        "label_counts = np.array([np.count_nonzero(adjusted_y_train == label) for label in unique_labels])\n",
        "relative_distribution = label_counts / len(adjusted_y_train)\n",
        "\n",
        "print(\"The gini-coefficient of the adjusted trainingsdata =\")\n",
        "calculate_gini(relative_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbceaem1bS-5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        requests.get('https://www.google.com')\n",
        "        print(\"Kept alive.\")\n",
        "    except:\n",
        "        print(\"Failed to keep alive.\")\n",
        "    time.sleep(600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "admVhzBHLnm8"
      },
      "source": [
        "# **fs_utilis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BOujKH2ohnM"
      },
      "source": [
        "These functions provide different feature selection methods based on node strength and node degree in input matrices W1 and W2. The specific method is chosen based on the value of the method parameter passed to the feature_selection function.\n",
        "\n",
        "\n",
        "This code is an adjustment of the existing code that stems from: Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWN9fJsOLqEk"
      },
      "outputs": [],
      "source": [
        "def feature_selection(W1, W2, k, method):\n",
        "    if method == \"Node Strength(in)\":\n",
        "        indices =  fs_strength_w(sparse.csr_matrix.transpose(W1), k)\n",
        "    elif method == \"Node Strength(out)\":\n",
        "        indices =  fs_strength_w(W2, k)\n",
        "    elif method == \"Node Degree(in)\":\n",
        "        indices =  fs_degree_w(sparse.csr_matrix.transpose(W1), k)\n",
        "    elif method == \"Node Degree(out)\":\n",
        "        indices =  fs_degree_w(W2, k)\n",
        "    elif method == \"Degree(in_out)\":\n",
        "        indices = fs_degree12(sparse.csr_matrix.transpose(W1), W2, k)\n",
        "    elif method == \"Degree(out_in)\":\n",
        "        indices = fs_degree12(W2, sparse.csr_matrix.transpose(W1), k)\n",
        "    elif method == \"Node Strength(in_out)\":\n",
        "        indices = fs_strength12(sparse.csr_matrix.transpose(W1), W2, k)\n",
        "    elif method == \"Node Strength(out_in)\":\n",
        "        indices = fs_strength12(W2, sparse.csr_matrix.transpose(W1), k)\n",
        "\n",
        "    return indices\n",
        "\n",
        "def get_degree(W):\n",
        "    weights = W.tolil()\n",
        "    num_ws = np.zeros(W.shape[1])\n",
        "    for i in range(W.shape[1]):\n",
        "        num_ws[i] = len(np.sum(weights[:,i].data))\n",
        "    return num_ws\n",
        "\n",
        "def fs_strength_w(W, k):\n",
        "    weights = W.tolil()\n",
        "    abs_w = np.absolute(weights)\n",
        "    sum_abs_w = np.sum(abs_w, axis = 0)\n",
        "    new_sum_w = np.zeros(W.shape[1])\n",
        "    #print(W.shape)\n",
        "    new_sum_w = np.zeros(W.shape[1])\n",
        "    for i in range(W.shape[1]):\n",
        "        new_sum_w[i] = sum_abs_w[0,i]\n",
        "    indices = new_sum_w.argsort()[-k:][::-1]\n",
        "    return indices\n",
        "\n",
        "def fs_degree_w(W, k):\n",
        "    num_ws = np.zeros(W.shape[1])\n",
        "    for i in range(W.shape[1]):\n",
        "        num_ws[i] = len(W[:,i].data)\n",
        "    fs_indices = num_ws.argsort()[-k:][::-1]\n",
        "    return fs_indices\n",
        "\n",
        "def fs_degree12(W1, W2, k):\n",
        "    weights = W1.tolil()\n",
        "    num_ws1 = np.zeros(W1.shape[1])\n",
        "    for i in range(W1.shape[1]):\n",
        "        num_ws1[i] = len(np.sum(weights[:,i].data))\n",
        "\n",
        "    weights = W2.tolil()\n",
        "    num_ws2 = np.zeros(W2.shape[1])\n",
        "    for i in range(W2.shape[1]):\n",
        "        num_ws2[i] = len(np.sum(weights[:,i].data))\n",
        "\n",
        "    num_ws = num_ws1 + num_ws2\n",
        "    c = int(num_ws.shape[0]*2/3)\n",
        "    indices = num_ws.argsort()[-c:][::-1]\n",
        "\n",
        "    weights = W1.tolil()\n",
        "    num_ws = np.zeros(W1.shape[1])\n",
        "    for i in range(W1.shape[1]):\n",
        "        num_ws[i] = len(np.sum(weights[indices,i].data))\n",
        "    fs_indices = num_ws.argsort()[-k:][::-1]\n",
        "    return fs_indices\n",
        "\n",
        "\n",
        "def fs_strength12(W1, W2, k):\n",
        "\n",
        "    weights = W1.tolil()\n",
        "    abs_w = np.absolute(weights)\n",
        "    sum_abs_w = np.sum(abs_w, axis = 1)\n",
        "    sum_abs_w = np.transpose(sum_abs_w)\n",
        "    new_sum_w1 = np.zeros(W1.shape[0])\n",
        "    for i in range(W1.shape[0]):\n",
        "        new_sum_w1[i] = sum_abs_w[0,i]\n",
        "\n",
        "    weights = W2.tolil()\n",
        "    abs_w = np.absolute(weights)\n",
        "    sum_abs_w = np.sum(abs_w, axis = 1)\n",
        "    sum_abs_w = np.transpose(sum_abs_w)\n",
        "    new_sum_w2 = np.zeros(W2.shape[0])\n",
        "    for i in range(W2.shape[0]):\n",
        "        new_sum_w2[i] = sum_abs_w[0,i]\n",
        "\n",
        "    new_sum_w = new_sum_w1 + new_sum_w1\n",
        "    c = int(new_sum_w.shape[0]*0.5)\n",
        "    indices1 = new_sum_w.argsort()[-c:][::-1]\n",
        "\n",
        "    weights = W1.tolil()\n",
        "    abs_w = np.absolute(weights)\n",
        "    sum_abs_w = np.sum(abs_w[indices1,:], axis = 0)\n",
        "    sum_abs_w = np.transpose(sum_abs_w)\n",
        "    new_sum_w = np.zeros(W1.shape[1])\n",
        "\n",
        "    for i in range(W1.shape[1]):\n",
        "        new_sum_w[i] = sum_abs_w[i,0]\n",
        "\n",
        "    indices = new_sum_w.argsort()[-k:][::-1]\n",
        "\n",
        "    return indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt168w-cJkjn"
      },
      "source": [
        "# **QuickSelection**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJluoDRfsGy"
      },
      "source": [
        "\"cacc\" stands for \"**clustering accuracy**\" and represents the accuracy of the clustering results obtained after applying feature selection. It is the accuracy metric calculated by the evaluation function. Clustering is an unsupervised learning task that groups similar samples together based on their feature similarities. Unlike classification, clustering does not have predefined class labels. Therefore, the evaluation of clustering accuracy is relative to the clustering algorithm and the underlying data distribution. It is not directly comparable to classification accuracy.\n",
        "\n",
        "\"Etacc\" stands for \"**Extra Trees accuracy**\" and represents the accuracy of the classification results obtained using the Extra Trees Classifier. It measures how well the classifier can predict the class labels based on the selected features. A high ETacc indicates that the selected features are effective in distinguishing different classes, resulting in accurate classification.The *eval_subset* function evaluates the performance of a subset of features selected using the SDAE.\n",
        "\n",
        "This code is an adjustment of Atashgahi, Z., Sokar, G., van der Lee, T., Mocanu, E., Mocanu, D. C., Veldhuis, R., & Pechenizkiy, M. (2020). Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. arXiv preprint arXiv:2012.00560."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW9Ob09eKDSs"
      },
      "outputs": [],
      "source": [
        "def best_map(l1, l2):\n",
        "    \"\"\"\n",
        "    Permute labels of l2 to match l1 as much as possible\n",
        "    \"\"\"\n",
        "    if len(l1) != len(l2):\n",
        "        print(\"L1.shape must == L2.shape\")\n",
        "        exit(0)\n",
        "\n",
        "    label1 = np.unique(l1)\n",
        "    n_class1 = len(label1)\n",
        "\n",
        "    label2 = np.unique(l2)\n",
        "    n_class2 = len(label2)\n",
        "\n",
        "    n_class = max(n_class1, n_class2)\n",
        "    G = np.zeros((n_class, n_class))\n",
        "\n",
        "    for i in range(0, n_class1):\n",
        "        for j in range(0, n_class2):\n",
        "            ss = l1 == label1[i]\n",
        "            tt = l2 == label2[j]\n",
        "            G[i, j] = np.count_nonzero(ss & tt)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(-G)\n",
        "\n",
        "    new_l2 = np.zeros(l2.shape)\n",
        "    for i in range(len(col_ind)):\n",
        "        new_l2[l2 == label2[col_ind[i]]] = label1[row_ind[i]]\n",
        "\n",
        "    return new_l2.astype(int)\n",
        "\n",
        "\n",
        "def evaluation(X_selected, n_clusters, y):\n",
        "    \"\"\"\n",
        "    This function calculates ARI, ACC and NMI of clustering results\n",
        "    Input\n",
        "    -----\n",
        "    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n",
        "            input data on the selected features\n",
        "    n_clusters: {int}\n",
        "            number of clusters\n",
        "    y: {numpy array}, shape (n_samples,)\n",
        "            true labels\n",
        "    Output\n",
        "    ------\n",
        "    nmi: {float}\n",
        "        Normalized Mutual Information\n",
        "    acc: {float}\n",
        "        Accuracy\n",
        "    \"\"\"\n",
        "    k_means = KMeans(n_clusters=n_clusters)\n",
        "\n",
        "    k_means.fit(X_selected)\n",
        "    y_predict = k_means.labels_\n",
        "\n",
        "\n",
        "\n",
        "    # calculate ACC\n",
        "    y_permuted_predict = best_map(y, y_predict)\n",
        "    acc = accuracy_score(y, y_permuted_predict)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def eval_subset(train, test):\n",
        "    n_clusters = len(np.unique(train[2]))\n",
        "\n",
        "    clf = ExtraTreesClassifier(n_estimators = 50, n_jobs = -1)\n",
        "    clf.fit(train[0], train[2])\n",
        "    DTacc = float(clf.score(test[0], test[2]))\n",
        "\n",
        "    max_iters = 10\n",
        "    cacc = 0.0\n",
        "    for iter in range(max_iters):\n",
        "        acc = evaluation(train[0], n_clusters = n_clusters, y = train[2]) #We repeat the K-means algorithm 10 times and report the average clustering results since K-means may converge to a local optimal\n",
        "        cacc += acc / max_iters\n",
        "    return  DTacc,float(cacc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "methods = ['Node Strength(in)']\n",
        "path = \"/content/gdrive/MyDrive/results/\"+dataset_name+\"/\"\n",
        "#print(\"Data = \", dataset_name)\n",
        "#X_train, Y_train, X_test, Y_test = load_data(dataset_name)\n",
        "#num_categories = len(set(Y_train))\n",
        "#gini, distribution = gini_algorithm(target_gini, num_categories)\n",
        "#adjusted_X_train, adjusted_y_train, adjusted_X_test, adjusted_y_test = imbalanced_data(X_train, Y_train, X_test, Y_test, distribution)\n",
        "\n",
        "\n",
        "print(\"Performing feature selection on \", dataset_name)\n",
        "print(\"Dataset shape:\")\n",
        "print(adjusted_X_train.shape)\n",
        "print(adjusted_y_train.shape)\n",
        "print(adjusted_X_test.shape)\n",
        "print(adjusted_y_test.shape)\n",
        "\n",
        "k =50\n",
        "\n",
        "for file in os.listdir(path):\n",
        "\n",
        "    cnt = 0\n",
        "    ls_acc = []\n",
        "    ls_acc2 = []\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    print(\"Quick_Selection results:\")\n",
        "    for run in os.listdir(path+file+\"/\"):\n",
        "        print(\"Run %d:\" % cnt, end = \"\")\n",
        "        cnt += 1\n",
        "        w1_10  = scipy.sparse.load_npz(path+file+\"/\"+run+\"/w1.npz\") # loads a NumPy-compatible sparse matrix saved in the compressed NPZ format.\n",
        "        w2_10  = scipy.sparse.load_npz(path+file+\"/\"+run+\"/w2.npz\") #\n",
        "        for j in range(len(methods)):\n",
        "            indices  = feature_selection(w1_10, w2_10, k, methods[j])\n",
        "            DTacc, cacc = eval_subset([adjusted_X_train[:,indices], adjusted_X_train, adjusted_y_train.ravel()],\n",
        "                            [adjusted_X_test[:, indices], adjusted_X_test,  adjusted_y_test.ravel()])\n",
        "            print(\"ETacc=%.4f | cacc=%.4f |\" % ( DTacc,cacc))\n",
        "            ls_acc.append(cacc)\n",
        "            ls_acc2.append(DTacc)\n",
        "    ls_acc = np.array(ls_acc)\n",
        "    mean = np.mean(ls_acc)\n",
        "    std = np.std(ls_acc)\n",
        "    print(\"cacc | mean=%.4f |  std=%.4f \" % (mean, std))\n",
        "    ls_acc2 = np.array(ls_acc2)\n",
        "    mean = np.mean(ls_acc2)\n",
        "    std = np.std(ls_acc2)\n",
        "    print(\"Etacc | mean=%.4f |  std=%.4f\"   % (mean, std))\n",
        "    print(\"shape indices\", indices.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuEBqf_G2Xb1"
      },
      "outputs": [],
      "source": [
        "print(adjusted_X_train.shape, adjusted_y_train.shape)\n",
        "print(adjusted_X_test.shape, adjusted_y_test.shape)\n",
        "\n",
        "x = selector.get_support(indices= False)\n",
        "\n",
        "print(x) #784 items\n",
        "\n",
        "#hoe te zorgen dat x deel twee wordt van adjusted_X_train zodat die gebruikt kunnen worden voor de analyse hieronder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqUmS0l9xWyy"
      },
      "outputs": [],
      "source": [
        "features_AEC = selector.transform(adjusted_X_train)\n",
        "\n",
        "print(features_AEC[1])\n",
        "print(features_AEC.shape) #200 is het aantal features dat geselecteerd moest worden, 784 is het aantal features origineel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7T1-Binzn3T"
      },
      "outputs": [],
      "source": [
        "def evaluation(X_selected, n_clusters, y):\n",
        "    \"\"\"\n",
        "    This function calculates ARI, ACC and NMI of clustering results\n",
        "    Input\n",
        "    -----\n",
        "    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n",
        "            input data on the selected features\n",
        "    n_clusters: {int}\n",
        "            number of clusters\n",
        "    y: {numpy array}, shape (n_samples,)\n",
        "            true labels\n",
        "    Output\n",
        "    ------\n",
        "    nmi: {float}\n",
        "        Normalized Mutual Information\n",
        "    acc: {float}\n",
        "        Accuracy\n",
        "    \"\"\"\n",
        "    k_means = KMeans(n_clusters=n_clusters)\n",
        "\n",
        "    k_means.fit(X_selected)\n",
        "    y_predict = k_means.labels_\n",
        "\n",
        "\n",
        "    # calculate ACC\n",
        "    y_permuted_predict = best_map(y, y_predict)\n",
        "    acc = accuracy_score(y, y_permuted_predict)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_subset(train, test):\n",
        "    n_clusters = len(np.unique(train[2]))\n",
        "\n",
        "    clf = ExtraTreesClassifier(n_estimators = 50, n_jobs = -1)\n",
        "    clf.fit(train[0], train[2])\n",
        "    DTacc = float(clf.score(test[0], test[2]))\n",
        "\n",
        "    max_iters = 10\n",
        "    cacc = 0.0\n",
        "    for iter in range(max_iters):\n",
        "        acc = evaluation(train[0], n_clusters = n_clusters, y = train[2])\n",
        "        cacc += acc / max_iters\n",
        "    return  DTacc,float(cacc)\n",
        "\n",
        "DTacc, cacc = eval_subset([adjusted_X_train[:, selector.get_support(indices = True)], adjusted_X_train, adjusted_y_train.ravel()], [adjusted_X_test[:, selector.get_support(indices = True)], adjusted_X_test,  adjusted_y_test.ravel()])\n",
        "print(\"ETacc=%.4f | cacc=%.4f |\" % ( DTacc,cacc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dd7IGYURNN7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        requests.get('https://www.google.com')\n",
        "        print(\"Kept alive.\")\n",
        "    except:\n",
        "        print(\"Failed to keep alive.\")\n",
        "    time.sleep(600)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l36KEH1avu9x"
      },
      "outputs": [],
      "source": [
        "target_gini = 0.79\n",
        "#target_gini ranges from 0 till 0.8 in steps of 0.2\n",
        "dataset_name = 'MNIST'\n",
        "#could be 'MNIST' or 'coil20'\n",
        "\n",
        "epoch = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COo4O5HOvxuF",
        "outputId": "840d6655-6446-40f1-ed4a-20f2ec57a3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "from scipy.sparse import lil_matrix\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import dok_matrix\n",
        "#the \"sparseoperations\" Cython library was tested in Ubuntu 16.04. Please note that you may encounter some \"solvable\" issues if you compile it in Windows.\n",
        "#import sparseoperations\n",
        "import datetime\n",
        "\n",
        "from scipy import sparse\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import loadmat\n",
        "from scipy.io import savemat\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import urllib.request as urllib2\n",
        "import errno\n",
        "import os\n",
        "import sys; sys.path.append(os.getcwd())\n",
        "import argparse\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment #The linear_assignment function is deprecated in 0.21 and will be removed from 0.23, but sklearn.utils.linear_assignment_ can be replaced by scipy.optimize.linear_sum_assignment\n"
      ],
      "metadata": {
        "id": "A3WjPlYJvxw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gini(x):\n",
        "#given a certain distribution, this function calculates the gini coefficient\n",
        "   total = 0\n",
        "   for i, xi in enumerate(x[:-1], 1):\n",
        "      total += np.sum(np.abs(xi - x[i:]))\n",
        "      return total / (len(x) ** 2 * np.mean(x))\n",
        "\n",
        "def gini_algorithm(target_gini, num_categories):\n",
        "    # target_gini is the gini to be reached with the eventual distribution, num_categories is the number of categories in the eventual distribution\n",
        "    print(\"Start gini-process\")\n",
        "    # Set the initial lower and upper bounds for the first element\n",
        "    lower_bound = 0\n",
        "    upper_bound = 100\n",
        "\n",
        "    # Use binary search to find the appropriate value for the first element\n",
        "    while True:\n",
        "        mid = (lower_bound + upper_bound) / 2\n",
        "        x = np.array([mid] + [1] * (num_categories - 1))\n",
        "        gini_coeff = calculate_gini(x)\n",
        "\n",
        "        if abs(gini_coeff - target_gini) < 1e-3:\n",
        "            distribution = x / np.sum(x)\n",
        "            print(\"Relative distribution over num_categories:\", distribution)\n",
        "            gini_coeff = calculate_gini(distribution)\n",
        "            print(\"Gini coefficient:\", gini_coeff)\n",
        "            return gini_coeff, distribution\n",
        "\n",
        "        elif gini_coeff > target_gini:\n",
        "            upper_bound = mid\n",
        "        else:\n",
        "            lower_bound = mid\n"
      ],
      "metadata": {
        "id": "NfS4-g5qv7TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dd8Vn7OTv-n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(name):\n",
        "\n",
        "    if name == \"coil20\":\n",
        "        mat = scipy.io.loadmat(\"/content/gdrive/MyDrive/datasets/Coil20.mat\")\n",
        "        print(mat['images'])\n",
        "        X = mat['images']\n",
        "        y = mat['labels']\n",
        "        y = np.array([int(label[3:]) for label in y])\n",
        "        print(\"labels\", y)\n",
        "\n",
        "        num_samples, height, width = X.shape\n",
        "        X = X.reshape((num_samples, height * width))\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        print(\"type X_train\", type(X_train))\n",
        "        print(\"type y_train\", type(y_train))\n",
        "        print(\"first element X_train\", X_train[0])\n",
        "        print(\"first element y_train\", y_train[0])\n",
        "\n",
        "    elif name == \"MNIST\":\n",
        "        import tensorflow as tf\n",
        "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "        #The reshaping operation converts the images from a grid of pixels to a vector representation\n",
        "        X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "        X_test  = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "        X_train = X_train.astype('float32')\n",
        "        X_test  = X_test.astype('float32')\n",
        "        #This scaler object is used to standardize the data based on the mean and standard deviation of the training set.\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        print(\"type X_train\", type(X_train))\n",
        "        print(\"type y_train\", type(y_train))\n",
        "\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def check_path(filename):\n",
        "    import os\n",
        "    if not os.path.exists(os.path.dirname(filename)):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(filename))\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise\n"
      ],
      "metadata": {
        "id": "guKGjR-pwFzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def imbalanced_data(X_train, y_train, X_test, y_test, distribution, max_iterations=25):\n",
        "    # Get the number of classes\n",
        "    num_classes = len(distribution)\n",
        "\n",
        "    # Create a dictionary to store the indices of instances for each class\n",
        "    class_indices_y_train = {class_label: np.where(y_train == class_label)[0] for class_label in np.unique(y_train)}\n",
        "    # Get the count of instances in each class based on the desired distribution\n",
        "    original_counts_y_train = np.array([len(class_indices_y_train[class_label]) for class_label in np.unique(y_train)])\n",
        "    # Initialize the adjusted counts with the original counts\n",
        "    adjusted_counts_y_train = original_counts_y_train.copy()\n",
        "    print(\"original_counts_y_train\", adjusted_counts_y_train)\n",
        "    # Calculate the target counts based on the desired distribution\n",
        "    target_counts_y_train = np.round(distribution * np.sum(original_counts_y_train)).astype(int)\n",
        "    # Limit the target counts to not exceed the original counts\n",
        "    target_counts_y_train = np.minimum(target_counts_y_train, original_counts_y_train)\n",
        "    print(\"target_counts _y_train\", target_counts_y_train)\n",
        "\n",
        "    # Create new empty lists to store the adjusted data during each iteration\n",
        "    adjusted_X_train_iteration = []\n",
        "    adjusted_y_train_iteration = []\n",
        "    adjusted_X_train = X_train\n",
        "    adjusted_y_train = y_train\n",
        "\n",
        "    # --------------------- Iteration adjustment training data ---------------------\n",
        "    # Iteratively adjust the dataset until the desired distribution is reached or the maximum number of iterations is reached\n",
        "    iteration = 0\n",
        "    while not np.allclose(adjusted_counts_y_train / np.sum(adjusted_counts_y_train), distribution,\n",
        "                          atol=1e-2) and int(iteration) < int(max_iterations):\n",
        "\n",
        "        print(\"WHILE LOOP FOR TRAINING DATA STARTED ITERATION:\", iteration)\n",
        "\n",
        "        adjusted_X_train = np.array(adjusted_X_train)\n",
        "        adjusted_y_train = np.array(adjusted_y_train)\n",
        "\n",
        "        class_indices_y_train = {class_label: np.where(adjusted_y_train == class_label)[0] for class_label in np.unique(adjusted_y_train)}\n",
        "        target_counts_y_train = np.round(distribution * 6634).astype(int)  # Set target counts to 6634\n",
        "        # Limit the target counts to not exceed the original counts\n",
        "        target_counts_y_train = np.minimum(target_counts_y_train, adjusted_counts_y_train)\n",
        "        print(\"target_counts_y_train loop\", target_counts_y_train)\n",
        "\n",
        "        adjusted_X_train_iteration.clear()  # Clear the list for each iteration\n",
        "        adjusted_y_train_iteration.clear()  # Clear the list for each iteration\n",
        "\n",
        "        for class_label in np.unique(adjusted_y_train):\n",
        "            class_indices_for_label_y_train = class_indices_y_train[class_label]\n",
        "            class_indices_sampled_y_train = np.random.choice(class_indices_for_label_y_train, size=target_counts_y_train[class_label], replace=False)\n",
        "            adjusted_X_train_iteration.extend(adjusted_X_train[class_indices_sampled_y_train])\n",
        "            adjusted_y_train_iteration.extend(adjusted_y_train[class_indices_sampled_y_train])\n",
        "\n",
        "        # Update the adjusted counts based on the adjusted dataset\n",
        "        adjusted_counts_y_train = np.array(\n",
        "            [len([label for label in adjusted_y_train_iteration if label == class_label]) for class_label in\n",
        "            np.unique(adjusted_y_train)])\n",
        "        print(\"adjusted_counts_y_train\", adjusted_counts_y_train)\n",
        "\n",
        "        # Update the main adjusted data lists with the iteration-adjustedlists\n",
        "        adjusted_X_train = adjusted_X_train_iteration.copy()\n",
        "        adjusted_y_train = adjusted_y_train_iteration.copy()\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(\"WHILE LOOP FOR TRAINING DATA STOPPED AT ITERATION:\", iteration)\n",
        "    print(\"adjusted_counts_y_train / np.sum(adjusted_counts_y_train)\", adjusted_counts_y_train / np.sum(adjusted_counts_y_train))\n",
        "    print(\"distribution\", distribution)\n",
        "\n",
        "    adjusted_X_train = np.array(adjusted_X_train)\n",
        "    adjusted_y_train = np.array(adjusted_y_train)\n",
        "    adjusted_X_test = np.array(X_test)\n",
        "    adjusted_y_test = np.array(y_test)\n",
        "\n",
        "    print(\"Number of cases in adjusted_y_train:\", adjusted_y_train.shape[0])\n",
        "    print(\"Number of cases in adjusted_y_test:\", adjusted_y_test.shape[0])\n",
        "\n",
        "    return adjusted_X_train, adjusted_y_train, adjusted_X_test, adjusted_y_test\n"
      ],
      "metadata": {
        "id": "23A0-S3ovxz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data(dataset_name)\n",
        "print(\"1. Data loaded\")\n",
        "num_categories = len(set(Y_train))\n",
        "gini, distribution = gini_algorithm(target_gini, num_categories)\n",
        "print(\"2. Gini_algorithm done\")\n",
        "adjusted_X_train, adjusted_y_train, adjusted_X_test, adjusted_y_test = imbalanced_data(X_train, Y_train, X_test, Y_test, distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D-W7fOWwQC_",
        "outputId": "38242afb-2fc9-4401-a76a-81e42ddd51b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type X_train <class 'numpy.ndarray'>\n",
            "type y_train <class 'numpy.ndarray'>\n",
            "1. Data loaded\n",
            "Start gini-process\n",
            "Relative distribution over num_categories: [0.89082638 0.0121304  0.0121304  0.0121304  0.0121304  0.0121304\n",
            " 0.0121304  0.0121304  0.0121304  0.0121304 ]\n",
            "Gini coefficient: 0.7908263836239575\n",
            "2. Gini_algorithm done\n",
            "original_counts_y_train [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
            "target_counts _y_train [5923  728  728  728  728  728  728  728  728  728]\n",
            "WHILE LOOP FOR TRAINING DATA STARTED ITERATION: 0\n",
            "target_counts_y_train loop [5910   80   80   80   80   80   80   80   80   80]\n",
            "adjusted_counts_y_train [5910   80   80   80   80   80   80   80   80   80]\n",
            "WHILE LOOP FOR TRAINING DATA STOPPED AT ITERATION: 1\n",
            "adjusted_counts_y_train / np.sum(adjusted_counts_y_train) [0.89140271 0.01206637 0.01206637 0.01206637 0.01206637 0.01206637\n",
            " 0.01206637 0.01206637 0.01206637 0.01206637]\n",
            "distribution [0.89082638 0.0121304  0.0121304  0.0121304  0.0121304  0.0121304\n",
            " 0.0121304  0.0121304  0.0121304  0.0121304 ]\n",
            "Number of cases in adjusted_y_train: 6630\n",
            "Number of cases in adjusted_y_test: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the modified dataset\n",
        "amount_of_cases = adjusted_y_train.shape[0]\n",
        "\n",
        "# Construct the new filename\n",
        "filename = f\"/content/gdrive/MyDrive/datasets/MNIST_{target_gini}.npz\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "# Save the modified dataset\n",
        "np.savez(filename, X_train=adjusted_X_train, y_train=adjusted_y_train, X_test=adjusted_X_test, y_test=adjusted_y_test)\n",
        "\n",
        "print(\"Modified dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW3ZGpmoxyG3",
        "outputId": "c6a9ab22-95cf-4c14-8ff4-c3bb1633fee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified dataset saved successfully.\n"
          ]
        }
      ]
    }
  ]
}